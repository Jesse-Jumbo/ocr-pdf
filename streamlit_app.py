#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OCR Webæ‡‰ç”¨ - åŸºæ–¼Streamlitçš„PDF OCRè™•ç†å¹³å°
"""

import streamlit as st
import os
import json
import tempfile
import zipfile
from pathlib import Path
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
# OCRè™•ç†å™¨é¡åˆ¥
import cv2
import numpy as np
from PIL import Image
import pdf2image
from paddleocr import PaddleOCR
import pytesseract
from typing import List, Dict, Any, Tuple
import logging
import re

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OCRProcessor:
    def __init__(self):
        """åˆå§‹åŒ–OCRè™•ç†å™¨"""
        # å»¶é²åˆå§‹åŒ–PaddleOCRï¼Œé¿å…å¤šåŸ·è¡Œç·’è¡çª
        self.paddle_ocr = None
        self._paddle_initialized = False
        self._batch_size = 3  # æ‰¹æ¬¡è™•ç†å¤§å°
        
        # è¨­ç½®Tesseractè·¯å¾‘ (è‡ªå‹•æª¢æ¸¬)
        import shutil
        tesseract_path = shutil.which('tesseract')
        if tesseract_path:
            pytesseract.pytesseract.tesseract_cmd = tesseract_path
        else:
            # å¸¸è¦‹è·¯å¾‘
            possible_paths = [
                '/opt/homebrew/bin/tesseract',  # macOS Homebrew
                '/usr/local/bin/tesseract',     # macOS/Linux
                '/usr/bin/tesseract',           # Linux
                'tesseract'                     # ç³»çµ±PATH
            ]
            for path in possible_paths:
                if shutil.which(path):
                    pytesseract.pytesseract.tesseract_cmd = path
                    break
        
        # æ–‡æœ¬æ–¹å‘æª¢æ¸¬åƒæ•¸
        self.vertical_threshold = 0.7  # ç›´å¼æ–‡æœ¬é–¾å€¼
        
        # æ–‡æœ¬åˆ†é¡è¦å‰‡
        self.text_classification_rules = {
            'title': {
                'keywords': ['æ¨™é¡Œ', 'é¡Œç›®', 'ä¸»é¡Œ', 'åç¨±', 'é¡Œ', 'ç¬¬.*ç« ', 'ç¬¬.*ç¯€'],
                'position_threshold': 0.1,  # ä½æ–¼é é¢é ‚éƒ¨
                'font_size_threshold': 1.5  # å­—é«”è¼ƒå¤§
            },
            'caption': {
                'keywords': ['åœ–', 'è¡¨', 'èªªæ˜', 'æ³¨é‡‹', 'å‚™è¨»', 'åœ–.*èªªæ˜', 'è¡¨.*èªªæ˜'],
                'position_threshold': 0.8,  # ä½æ–¼é é¢åº•éƒ¨
                'font_size_threshold': 0.8  # å­—é«”è¼ƒå°
            },
            'table': {
                'keywords': ['è¡¨', 'è¡¨æ ¼', 'çµ±è¨ˆ', 'æ•¸æ“š', 'é …ç›®', 'å…§å®¹'],
                'pattern': r'\|.*\|',  # åŒ…å«è¡¨æ ¼ç¬¦è™Ÿ
                'line_count_threshold': 3  # å¤šè¡Œæ–‡æœ¬
            },
            'content': {
                'default': True  # é»˜èªç‚ºå…§æ–‡
            }
        }
        
        # åŸ·è¡Œç·’é–ï¼Œç¢ºä¿PaddleOCRåˆå§‹åŒ–æ˜¯åŸ·è¡Œç·’å®‰å…¨çš„
        import threading
        self._init_lock = threading.Lock()
    
    def _init_paddle_ocr(self):
        """å»¶é²åˆå§‹åŒ–PaddleOCRï¼Œç¢ºä¿åŸ·è¡Œç·’å®‰å…¨"""
        if not self._paddle_initialized:
            with self._init_lock:
                if not self._paddle_initialized:
                    # åœ¨Streamlit Cloudä¸Šå®Œå…¨ç¦ç”¨PaddleOCRï¼Œé¿å…CPUæŒ‡ä»¤é›†å•é¡Œ
                    logger.info("åœ¨Streamlit Cloudç’°å¢ƒä¸­ï¼Œå°‡åªä½¿ç”¨Tesseracté€²è¡ŒOCR")
                    self.paddle_ocr = None
                    self._paddle_initialized = True
    
    def _cleanup_memory(self):
        """æ¸…ç†è¨˜æ†¶é«”ï¼Œé¿å…ç´¯ç©éå¤š"""
        import gc
        gc.collect()
        if hasattr(self, 'paddle_ocr') and self.paddle_ocr:
            # æ¸…ç†PaddleOCRå…§éƒ¨å¿«å–
            try:
                del self.paddle_ocr
                self.paddle_ocr = None
                self._paddle_initialized = False
            except:
                pass

    def process_single_page(self, page_data, enable_postprocess=True):
        """è™•ç†å–®ä¸€é é¢ - ç”¨æ–¼æ‰¹æ¬¡è™•ç†ï¼ˆé¿å…Streamlit contextå•é¡Œï¼‰"""
        page_num, image, page_height = page_data
        
        # ç¢ºä¿PaddleOCRå·²åˆå§‹åŒ–
        self._init_paddle_ocr()
        
        # æª¢æ¸¬æ¯å€‹æ–‡æœ¬å¡Šçš„æ–¹å‘ï¼ˆåœ–åƒå·²é è™•ç†ï¼‰
        text_blocks = self.detect_text_direction_per_block(image)
        
        # å¦‚æœPaddleOCRä¸å¯ç”¨ï¼Œä½¿ç”¨å¢å¼·ç‰ˆTesseract
        if self.paddle_ocr is None and not text_blocks:
            logger.info("ä½¿ç”¨å¢å¼·ç‰ˆTesseracté€²è¡ŒOCR")
            # ä½¿ç”¨å¢å¼·ç‰ˆTesseract
            tesseract_results = self.extract_text_tesseract_enhanced(image, "horizontal")
            
            # è½‰æ›ç‚ºtext_blocksæ ¼å¼
            text_blocks = []
            for result in tesseract_results:
                width = result["position"]["width"]
                height = result["position"]["height"]
                
                if height > 0:
                    aspect_ratio = width / height
                    direction = "vertical" if aspect_ratio < self.vertical_threshold else "horizontal"
                else:
                    direction = "horizontal"
                
                text_blocks.append({
                    "text": result["text"],
                    "confidence": result["confidence"],
                    "bbox": [],  # Tesseractä¸æä¾›bbox
                    "direction": direction,
                    "position": result["position"]
                })
        
        # åˆ†é¡æ¯å€‹æ–‡æœ¬å¡Š
        classified_blocks = []
        
        for block in text_blocks:
            # æ–‡å­—å¾Œè™•ç†ï¼ˆè¾­å…¸ç³¾éŒ¯/æ­£è¦åŒ–ï¼‰
            clean_text = self.correct_text(block["text"]) if enable_postprocess else block["text"]
            
            # åˆ†é¡æ–‡æœ¬é¡å‹
            text_type = self.classify_text_type(block, page_height)
            
            # ç°¡åŒ–çš„æ–‡æœ¬å¡Šçµæ§‹
            simplified_block = {
                "text": clean_text,
                "type": text_type,
                "direction": block["direction"],
                "confidence": block["confidence"]
            }
            classified_blocks.append(simplified_block)
        
        # æŒ‰é–±è®€é †åºçµ„ç¹”
        if classified_blocks:
            if any(block["direction"] == "vertical" for block in classified_blocks):
                # æ··åˆæ–¹å‘ï¼ŒæŒ‰ä½ç½®æ’åº
                organized_texts = sorted(classified_blocks, 
                                       key=lambda x: (x.get("position", {}).get("y", 0), 
                                                    x.get("position", {}).get("x", 0)))
            else:
                # ç´”æ©«å¼ï¼ŒæŒ‰è¡Œæ’åº
                organized_texts = sorted(classified_blocks, 
                                       key=lambda x: (x.get("position", {}).get("y", 0), 
                                                    x.get("position", {}).get("x", 0)))
        else:
            organized_texts = []
        
        # æ§‹å»ºé é¢çµæœ
        page_result = {
            "page_number": page_num,
            "text_blocks": organized_texts,
            "full_text": "\n".join([block["text"] for block in organized_texts])
        }
        
        return page_result, organized_texts
        
    def pdf_to_images(self, pdf_path: str, dpi: int = 300) -> List[np.ndarray]:
        """å°‡PDFè½‰æ›ç‚ºé«˜è³ªé‡åœ–åƒ"""
        logger.info(f"æ­£åœ¨è½‰æ›PDF: {pdf_path}")
        try:
            # ä½¿ç”¨æ›´é«˜çš„DPIå’Œæ›´å¥½çš„è½‰æ›åƒæ•¸
            images = pdf2image.convert_from_path(
                pdf_path, 
                dpi=dpi,
                first_page=None,
                last_page=None,
                fmt='RGB',
                thread_count=4,
                poppler_path=None,
                grayscale=False,
                size=None,
                transparent=False,
                single_file=False,
                output_file=None,
                jpegopt=None,
                strict=False,
                use_pdftocairo=False,
                timeout=600
            )
            return [np.array(img) for img in images]
        except Exception as e:
            logger.error(f"PDFè½‰æ›å¤±æ•—: {e}")
            return []
    
    def detect_text_direction_per_block(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """æª¢æ¸¬æ¯å€‹æ–‡æœ¬å¡Šçš„æ–¹å‘ (ç›´å¼/æ©«å¼)"""
        # å¦‚æœPaddleOCRä¸å¯ç”¨ï¼Œä½¿ç”¨Tesseract
        if self.paddle_ocr is None:
            return self._detect_text_direction_tesseract(image)
        
        try:
            # ä½¿ç”¨PaddleOCRæª¢æ¸¬æ–‡æœ¬æ–¹å‘
            result = self.paddle_ocr.ocr(image, cls=True)
            
            if not result or not result[0]:
                return self._detect_text_direction_tesseract(image)
            
            text_blocks = []
            for line in result[0]:
                if len(line) >= 2:
                    bbox = line[0]
                    text = line[1][0]
                    confidence = line[1][1]
                    
                    # è¨ˆç®—æ–‡æœ¬æ¡†çš„é•·å¯¬æ¯”å’Œè§’åº¦
                    x_coords = [point[0] for point in bbox]
                    y_coords = [point[1] for point in bbox]
                    
                    width = max(x_coords) - min(x_coords)
                    height = max(y_coords) - min(y_coords)
                    
                    # åˆ¤æ–·æ–¹å‘
                    if height > 0:
                        aspect_ratio = width / height
                        direction = "vertical" if aspect_ratio < self.vertical_threshold else "horizontal"
                    else:
                        direction = "horizontal"
                    
                    text_blocks.append({
                        "text": text,
                        "confidence": confidence,
                        "bbox": bbox,
                        "direction": direction,
                        "position": {
                            "x": min(x_coords),
                            "y": min(y_coords),
                            "width": width,
                            "height": height
                        }
                    })
            
            return text_blocks
        except Exception as e:
            logger.error(f"PaddleOCRæª¢æ¸¬å¤±æ•—: {e}")
            return self._detect_text_direction_tesseract(image)
    
    def _detect_text_direction_tesseract(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Tesseractæª¢æ¸¬æ–‡æœ¬æ–¹å‘"""
        try:
            # ä½¿ç”¨Tesseractæª¢æ¸¬æ–‡æœ¬
            tesseract_results = self.extract_text_tesseract(image, "horizontal")
            
            text_blocks = []
            for result in tesseract_results:
                # ç°¡å–®çš„æ–¹å‘åˆ¤æ–·ï¼šåŸºæ–¼æ–‡æœ¬æ¡†çš„é•·å¯¬æ¯”
                width = result["position"]["width"]
                height = result["position"]["height"]
                
                if height > 0:
                    aspect_ratio = width / height
                    direction = "vertical" if aspect_ratio < self.vertical_threshold else "horizontal"
                else:
                    direction = "horizontal"
                
                text_blocks.append({
                    "text": result["text"],
                    "confidence": result["confidence"],
                    "bbox": [],  # Tesseractä¸æä¾›bbox
                    "direction": direction,
                    "position": result["position"]
                })
            
            return text_blocks
        except Exception as e:
            logger.error(f"Tesseractæ–¹å‘æª¢æ¸¬å¤±æ•—: {e}")
            return []
    
    def classify_text_type(self, text_block: Dict[str, Any], page_height: int) -> str:
        """åˆ†é¡æ–‡æœ¬é¡å‹ (æ¨™é¡Œã€å…§æ–‡ã€è¡¨æ ¼ã€åœ–ç‰‡èªªæ˜ç­‰)"""
        text = text_block["text"]
        position = text_block["position"]
        confidence = text_block["confidence"]
        
        # è¨ˆç®—ç›¸å°ä½ç½®
        relative_y = position["y"] / page_height if page_height > 0 else 0
        
        # è¨ˆç®—å­—é«”å¤§å° (åŸºæ–¼æ–‡æœ¬æ¡†é«˜åº¦)
        font_size = position["height"]
        
        # æª¢æŸ¥æ¨™é¡Œ
        title_rules = self.text_classification_rules['title']
        if (relative_y < title_rules['position_threshold'] and 
            font_size > title_rules['font_size_threshold'] * 20):  # 20æ˜¯åŸºæº–å­—é«”å¤§å°
            for keyword in title_rules['keywords']:
                if re.search(keyword, text):
                    return "title"
            # å¦‚æœä½ç½®å’Œå­—é«”ç¬¦åˆæ¨™é¡Œç‰¹å¾µï¼Œä¹Ÿæ­¸ç‚ºæ¨™é¡Œ
            if relative_y < 0.2 and font_size > 30:
                return "title"
        
        # æª¢æŸ¥åœ–ç‰‡èªªæ˜
        caption_rules = self.text_classification_rules['caption']
        if (relative_y > caption_rules['position_threshold'] and 
            font_size < caption_rules['font_size_threshold'] * 20):
            for keyword in caption_rules['keywords']:
                if re.search(keyword, text):
                    return "caption"
        
        # æª¢æŸ¥è¡¨æ ¼
        table_rules = self.text_classification_rules['table']
        if re.search(table_rules['pattern'], text):
            return "table"
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼é—œéµè©
        for keyword in table_rules['keywords']:
            if re.search(keyword, text):
                return "table"
        
        # é»˜èªç‚ºå…§æ–‡
        return "content"
    
    def preprocess_image(self, image: np.ndarray, direction: str, scale: float = 1.0) -> np.ndarray:
        """Chromeç´šåˆ¥åœ–åƒé è™•ç†ï¼ˆå«å¯é¸è¶…è§£æï¼‰"""
        # 0. å¯é¸è¶…è§£æï¼ˆé«˜è³ªé‡ç‰ˆï¼‰ï¼šLANCZOS4æ’å€¼ + éŠ³åŒ–
        if scale and scale > 1.0:
            new_w = int(image.shape[1] * scale)
            new_h = int(image.shape[0] * scale)
            image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
            # ä½¿ç”¨æ›´å¼·çš„éŠ³åŒ–æ ¸
            kernel = np.array([[-1,-1,-1], [-1,12,-1], [-1,-1,-1]])
            image = cv2.filter2D(image, -1, kernel)
        
        # è½‰æ›ç‚ºç°åº¦åœ–
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image
        
        # 1. é«˜ç´šå»å™ª - ä½¿ç”¨éå±€éƒ¨å‡å€¼å»å™ª
        denoised = cv2.fastNlMeansDenoising(gray, None, h=10, templateWindowSize=7, searchWindowSize=21)
        
        # 2. å°æ¯”åº¦å¢å¼· - ä½¿ç”¨æ›´å¼·çš„CLAHE
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(denoised)
        
        # 3. éŠ³åŒ– - ä½¿ç”¨æ›´å¼·çš„éŠ³åŒ–æ ¸
        kernel = np.array([[-1,-1,-1], [-1,12,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(enhanced, -1, kernel)
        
        # 4. é‚Šç·£å¢å¼·
        edges = cv2.Canny(sharpened, 50, 150)
        sharpened = cv2.addWeighted(sharpened, 0.8, edges, 0.2, 0)
        
        # 5. è‡ªé©æ‡‰äºŒå€¼åŒ– - ä½¿ç”¨æ›´ç²¾ç¢ºçš„åƒæ•¸
        binary = cv2.adaptiveThreshold(
            sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 15, 3
        )
        
        # 6. å½¢æ…‹å­¸æ“ä½œ - æ›´ç²¾ç´°çš„è™•ç†
        # å…ˆé–‰é‹ç®—é€£æ¥æ–‡å­—
        kernel_close = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel_close)
        
        # å†é–‹é‹ç®—å»é™¤å™ªé»
        kernel_open = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_open)
        
        # 7. å¦‚æœæ˜¯ç›´å¼æ–‡æœ¬ï¼Œå˜—è©¦æ—‹è½‰
        if direction == "vertical":
            binary = self.rotate_if_needed(binary)
        
        return binary

    def correct_text(self, text: str) -> str:
        """å­—å…ƒæ­£è¦åŒ–èˆ‡è¾­å…¸å¼ç³¾éŒ¯ï¼ˆç¹é«”ä¸­æ–‡å„ªåŒ–ç‰ˆï¼‰"""
        if not text:
            return text
        # 1) Unicode æ­£è¦åŒ–
        import unicodedata, re as _re
        t = unicodedata.normalize('NFKC', text)
        
        # 2) å¸¸è¦‹OCRéŒ¯èª¤ä¿®æ­£ï¼ˆç¹é«”ä¸­æ–‡ï¼‰
        substitutions = {
            'ã€‚Â·': 'ã€‚', 'Â·ã€‚': 'ã€‚', 'â€¦': 'â€¦', 'â€”': '-', 'â€“': '-',
            'åœ–æ›¸ç¦®å·': 'åœ–æ›¸ç¦®åˆ¸', 'ç¦®å·': 'ç¦®åˆ¸',
            'â—‹': 'â—‹',
            # ç¹é«”ä¸­æ–‡å¸¸è¦‹OCRéŒ¯èª¤
            'éƒ¨æš‘': 'éƒ¨ç½²', 'è®¿å•': 'è¨ªå•', 'å‡†': 'æº–å‚™', 'é£Ÿåº“': 'å€‰åº«',
            'é˜²é–“': 'è¨ªå•', 'å¸è™Ÿ': 'å¸³è™Ÿ', 'é»æ“': 'é»æ“Š', 'é¸': 'é¸æ“‡',
            'è¨­ç½®': 'è¨­ç½®', 'ç­‰å¾…': 'ç­‰å¾…', 'ç²å¾—': 'ç²å¾—', 'è®¿å•': 'è¨ªå•',
            'é¸é ‚': 'é¸é …', 'æ¨è”—': 'æ¨è–¦', 'é˜²å‘¨': 'è¨ªå•', 'ç›’åº“': 'å€‰åº«',
            'å®‰è£…': 'å®‰è£', 'å‰µå»º': 'å‰µå»º', 'æ³¨æ„äº‹é ‚': 'æ³¨æ„äº‹é …',
            'å…è´¹': 'å…è²»', 'å»ºä¸Šå‚…': 'å»ºè­°ä¸Šå‚³', 'è¾ƒé•·': 'è¼ƒé•·',
            'ç™¼é™åˆ¶': 'ä½¿ç”¨é™åˆ¶', 'æŸ¥': 'æª¢æŸ¥', 'æ£€æŸ¥': 'æª¢æŸ¥',
            'ä¸‹è½½': 'ä¸‹è¼‰', 'æŸ¥': 'æª¢æŸ¥', 'æ—¥ç¿°å‡º': 'æ—¥èªŒè¼¸å‡º',
            'æ¯”è¾ƒ': 'æ¯”è¼ƒ', 'é¢åº¦': 'é¡åº¦', 'ä»˜è²»': 'ä»˜è²»', 'æ¨åº¦': 'æ¨è–¦',
            'æ‡‰ç”¨': 'æ‡‰ç”¨', 'é™åˆ¶': 'é™åˆ¶', 'æœˆ': 'æœˆ'
        }
        for a, b in substitutions.items():
            t = t.replace(a, b)
        
        # 3) æ•¸å­—/è‹±æ–‡å­—æ··æ·†
        t = _re.sub(r'(?<=\d)[Oo](?=\d)', '0', t)
        t = _re.sub(r'(?<=\d)[lI](?=\d)', '1', t)
        
        # 4) ç§»é™¤æ³•æ–‡å’Œäº‚ç¢¼ï¼ˆå¸¸è¦‹OCRéŒ¯èª¤ï¼‰
        t = _re.sub(r'[A-Z\s]+[A-Z][A-Z\s]+', '', t)  # ç§»é™¤æ³•æ–‡æ¨¡å¼
        t = _re.sub(r'BTS.*?EIFFEL', '', t)  # ç§»é™¤æ³•æ–‡å­¸æ ¡å
        t = _re.sub(r'PrOFesseur.*?rOS', '', t)  # ç§»é™¤æ³•æ–‡æ•™æˆå
        t = _re.sub(r'\d+/\d+', '', t)  # ç§»é™¤åˆ†æ•¸æ ¼å¼
        
        # 5) æ¸…ç†å¤šé¤˜ç©ºæ ¼å’Œæ›è¡Œ
        t = _re.sub(r'\s+', ' ', t).strip()
        
        # 6) ç°¡è½‰ç¹ï¼ˆè‹¥å¯ç”¨ä¸”å•Ÿç”¨ï¼‰
        if st.session_state.get('enable_text_postprocess', True):
            try:
                from opencc import OpenCC  # type: ignore
                t = OpenCC('s2t').convert(t)
            except Exception:
                pass
        return t
    
    def rotate_if_needed(self, image: np.ndarray) -> np.ndarray:
        """æª¢æ¸¬ä¸¦æ—‹è½‰åœ–åƒ"""
        # ä½¿ç”¨Tesseractæª¢æ¸¬è§’åº¦
        try:
            osd = pytesseract.image_to_osd(image, output_type=pytesseract.Output.DICT)
            angle = osd['rotate']
            if angle != 0:
                # æ—‹è½‰åœ–åƒ
                (h, w) = image.shape[:2]
                center = (w // 2, h // 2)
                M = cv2.getRotationMatrix2D(center, angle, 1.0)
                rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
                return rotated
        except:
            pass
        
        return image
    
    def extract_text_paddle(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ä½¿ç”¨PaddleOCRæå–æ–‡æœ¬"""
        if self.paddle_ocr is None:
            return []
        
        try:
            result = self.paddle_ocr.ocr(image, cls=True)
            
            if not result or not result[0]:
                return []
            
            extracted_texts = []
            for line in result[0]:
                if len(line) >= 2:
                    bbox = line[0]
                    text = line[1][0]
                    confidence = line[1][1]
                    
                    # è¨ˆç®—æ–‡æœ¬ä½ç½®
                    x_coords = [point[0] for point in bbox]
                    y_coords = [point[1] for point in bbox]
                    
                    extracted_texts.append({
                        "text": text,
                        "confidence": confidence,
                        "bbox": bbox,
                        "position": {
                            "x": min(x_coords),
                            "y": min(y_coords),
                            "width": max(x_coords) - min(x_coords),
                            "height": max(y_coords) - min(y_coords)
                        }
                    })
            
            return extracted_texts
        except Exception as e:
            logger.error(f"PaddleOCRæ–‡æœ¬æå–å¤±æ•—: {e}")
            return []
    
    def extract_text_tesseract(self, image: np.ndarray, direction: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é«˜é…ç½®Tesseractæå–æ–‡æœ¬ - ç¹é«”ä¸­æ–‡å„ªåŒ–"""
        # è¨­ç½®èªè¨€å’Œé…ç½® - å„ªå…ˆç¹é«”ä¸­æ–‡
        lang = 'chi_tra+chi_sim+eng'  # ç¹é«”ä¸­æ–‡+ç°¡é«”ä¸­æ–‡+è‹±æ–‡
        
        # ä½¿ç”¨æ›´ç²¾ç¢ºçš„PSMæ¨¡å¼
        if direction == "vertical":
            # ç›´å¼æ–‡æœ¬é…ç½® - ä½¿ç”¨å–®è¡Œæ–‡æœ¬æ¨¡å¼
            config = '--psm 7 -c preserve_interword_spaces=1 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å„„é›¶å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬å„„'
        else:
            # æ©«å¼æ–‡æœ¬é…ç½® - ä½¿ç”¨è‡ªå‹•é é¢åˆ†å‰²æ¨¡å¼
            config = '--psm 1 -c preserve_interword_spaces=1 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å„„é›¶å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬å„„'
        
        try:
            # ç²å–è©³ç´°ä¿¡æ¯
            data = pytesseract.image_to_data(image, lang=lang, config=config, output_type=pytesseract.Output.DICT)
            
            extracted_texts = []
            n_boxes = len(data['text'])
            
            for i in range(n_boxes):
                text = data['text'][i].strip()
                confidence = int(data['conf'][i])
                
                # å¤§å¹…é™ä½ç½®ä¿¡åº¦é–¾å€¼ï¼Œæª¢æ¸¬æ›´å¤šæ–‡å­—
                if text and confidence > 10 and len(text) > 0:
                    # éæ¿¾æ˜é¡¯çš„å™ªé»
                    if len(text) == 1 and confidence < 30:
                        continue
                    
                    extracted_texts.append({
                        "text": text,
                        "confidence": confidence / 100.0,
                        "position": {
                            "x": data['left'][i],
                            "y": data['top'][i],
                            "width": data['width'][i],
                            "height": data['height'][i]
                        }
                    })
            
            return extracted_texts
        except Exception as e:
            logger.error(f"Tesseract OCRå¤±æ•—: {e}")
            return []
    
    def extract_text_tesseract_enhanced(self, image: np.ndarray, direction: str) -> List[Dict[str, Any]]:
        """å¢å¼·ç‰ˆTesseract OCR - ä½¿ç”¨å¤šç¨®é…ç½®å˜—è©¦"""
        results = []
        
        # å˜—è©¦å¤šç¨®é…ç½®
        configs = [
            # é…ç½®1ï¼šæ¨™æº–é…ç½®
            '--psm 1 -c preserve_interword_spaces=1',
            # é…ç½®2ï¼šå–®åˆ—æ–‡æœ¬
            '--psm 4 -c preserve_interword_spaces=1',
            # é…ç½®3ï¼šå–®è¡Œæ–‡æœ¬
            '--psm 7 -c preserve_interword_spaces=1',
            # é…ç½®4ï¼šå–®è©
            '--psm 8 -c preserve_interword_spaces=1'
        ]
        
        lang = 'chi_tra+chi_sim+eng'
        
        for config in configs:
            try:
                data = pytesseract.image_to_data(image, lang=lang, config=config, output_type=pytesseract.Output.DICT)
                
                for i in range(len(data['text'])):
                    text = data['text'][i].strip()
                    confidence = int(data['conf'][i])
                    
                    if text and confidence > 20 and len(text) > 0:
                        # æª¢æŸ¥æ˜¯å¦èˆ‡ç¾æœ‰çµæœé‡ç–Š
                        is_duplicate = False
                        for existing in results:
                            if self._texts_overlap_simple(existing, {
                                "x": data['left'][i],
                                "y": data['top'][i],
                                "width": data['width'][i],
                                "height": data['height'][i]
                            }):
                                is_duplicate = True
                                break
                        
                        if not is_duplicate:
                            results.append({
                                "text": text,
                                "confidence": confidence / 100.0,
                                "position": {
                                    "x": data['left'][i],
                                    "y": data['top'][i],
                                    "width": data['width'][i],
                                    "height": data['height'][i]
                                }
                            })
            except Exception as e:
                logger.warning(f"Tesseracté…ç½® {config} å¤±æ•—: {e}")
                continue
        
        return results
    
    def _texts_overlap_simple(self, text1: Dict, pos2: Dict) -> bool:
        """ç°¡å–®çš„é‡ç–Šæª¢æ¸¬"""
        pos1 = text1["position"]
        
        # ç°¡å–®çš„é‡ç–Šæª¢æ¸¬
        overlap_x = not (pos1["x"] + pos1["width"] < pos2["x"] or pos2["x"] + pos2["width"] < pos1["x"])
        overlap_y = not (pos1["y"] + pos1["height"] < pos2["y"] or pos2["y"] + pos2["height"] < pos1["y"])
        
        return overlap_x and overlap_y
    
    def merge_ocr_results(self, paddle_results: List[Dict], tesseract_results: List[Dict]) -> List[Dict[str, Any]]:
        """åˆä½µPaddleOCRå’ŒTesseractçš„çµæœ"""
        merged = []
        
        # å„ªå…ˆä½¿ç”¨PaddleOCRçµæœï¼ˆå°ä¸­æ–‡æ”¯æŒæ›´å¥½ï¼‰
        for result in paddle_results:
            merged.append({
                "text": result["text"],
                "confidence": result["confidence"],
                "position": result["position"],
                "source": "paddle"
            })
        
        # è£œå……Tesseractçµæœï¼ˆå¦‚æœPaddleOCRæ²’æœ‰è­˜åˆ¥åˆ°ï¼‰
        for t_result in tesseract_results:
            # æª¢æŸ¥æ˜¯å¦èˆ‡ç¾æœ‰çµæœé‡ç–Š
            is_duplicate = False
            for m_result in merged:
                if self.texts_overlap(t_result, m_result):
                    is_duplicate = True
                    break
            
            if not is_duplicate and t_result["text"].strip():
                merged.append({
                    "text": t_result["text"],
                    "confidence": t_result["confidence"],
                    "position": t_result["position"],
                    "source": "tesseract"
                })
        
        return merged
    
    def texts_overlap(self, text1: Dict, text2: Dict) -> bool:
        """æª¢æŸ¥å…©å€‹æ–‡æœ¬æ˜¯å¦é‡ç–Š"""
        pos1 = text1["position"]
        pos2 = text2["position"]
        
        # ç°¡å–®çš„é‡ç–Šæª¢æ¸¬
        overlap_x = not (pos1["x"] + pos1["width"] < pos2["x"] or pos2["x"] + pos2["width"] < pos1["x"])
        overlap_y = not (pos1["y"] + pos1["height"] < pos2["y"] or pos2["y"] + pos2["height"] < pos1["y"])
        
        return overlap_x and overlap_y
    
    def organize_text_by_reading_order(self, texts: List[Dict], direction: str) -> List[Dict[str, Any]]:
        """æŒ‰é–±è®€é †åºçµ„ç¹”æ–‡æœ¬"""
        if direction == "vertical":
            # ç›´å¼æ–‡æœ¬ï¼šå¾å³åˆ°å·¦ï¼Œå¾ä¸Šåˆ°ä¸‹
            sorted_texts = sorted(texts, key=lambda x: (-x["position"]["x"], x["position"]["y"]))
        else:
            # æ©«å¼æ–‡æœ¬ï¼šå¾å·¦åˆ°å³ï¼Œå¾ä¸Šåˆ°ä¸‹
            sorted_texts = sorted(texts, key=lambda x: (x["position"]["y"], x["position"]["x"]))
        
        return sorted_texts
import pandas as pd

# é é¢é…ç½®
st.set_page_config(
    page_title="OCRæ–‡æœ¬è­˜åˆ¥ç³»çµ±",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾©CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        color: #1f77b4;
    }
    .sub-header {
        font-size: 1.2rem;
        text-align: center;
        margin-bottom: 2rem;
        color: #666;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.375rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 0.375rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .progress-container {
        background-color: #f8f9fa;
        border-radius: 0.375rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .file-info {
        background-color: #e9ecef;
        border-radius: 0.375rem;
        padding: 0.75rem;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

def init_session_state():
    """åˆå§‹åŒ–session state"""
    if 'ocr_processor' not in st.session_state:
        st.session_state.ocr_processor = None
    if 'processing_results' not in st.session_state:
        st.session_state.processing_results = {}
    if 'current_file' not in st.session_state:
        st.session_state.current_file = None
    if 'is_processing' not in st.session_state:
        st.session_state.is_processing = False
    if 'stop_processing' not in st.session_state:
        st.session_state.stop_processing = False

def load_ocr_processor():
    """è¼‰å…¥OCRè™•ç†å™¨"""
    if st.session_state.ocr_processor is None:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–OCRå¼•æ“..."):
            try:
                st.session_state.ocr_processor = OCRProcessor()
                st.success("OCRå¼•æ“åˆå§‹åŒ–å®Œæˆï¼")
            except Exception as e:
                st.error(f"OCRå¼•æ“åˆå§‹åŒ–å¤±æ•—: {str(e)}")
                return False
    return True

def process_pdf_file(uploaded_file, progress_bar, status_text, realtime_container):
    """è™•ç†ä¸Šå‚³çš„PDFæ–‡ä»¶ - æ”¯æŒå³æ™‚æ›´æ–°å’Œæš«åœ/åœæ­¢"""
    try:
        # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶åˆ°è‡¨æ™‚ç›®éŒ„
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        # æ›´æ–°ç‹€æ…‹
        status_text.text("æ­£åœ¨è½‰æ›PDFç‚ºåœ–åƒ...")
        progress_bar.progress(10)
        
        # è½‰æ›PDFç‚ºåœ–åƒ - æ·»åŠ é€²åº¦æ›´æ–°
        try:
            # ä½¿ç”¨é è¨­DPIæˆ–å¾session stateç²å–
            dpi = st.session_state.get('dpi', 300)
            images = st.session_state.ocr_processor.pdf_to_images(tmp_file_path, dpi=dpi)
            if not images:
                return None, "PDFè½‰æ›å¤±æ•—"
            
            # æ›´æ–°é€²åº¦
            progress_bar.progress(20)
            status_text.text(f"PDFè½‰æ›å®Œæˆï¼Œå…± {len(images)} é ï¼Œé–‹å§‹OCRè™•ç†...")
            
        except Exception as e:
            logger.error(f"PDFè½‰æ›éŒ¯èª¤: {e}")
            return None, f"PDFè½‰æ›å¤±æ•—: {str(e)}"
        
        total_pages = len(images)
        result = {
            "file_name": uploaded_file.name,
            "total_pages": total_pages,
            "pages": []
        }
        
        # é–‹å§‹è™•ç†
        status_text.text("é–‹å§‹è™•ç†PDFé é¢...")
        
        # æº–å‚™é é¢æ•¸æ“š - é è™•ç†åœ–åƒ
        page_data_list = []
        for page_num, image in enumerate(images, 1):
            page_height = image.shape[0]
            # é å…ˆæ‡‰ç”¨è¶…è§£æé è™•ç†
            processed_image = st.session_state.ocr_processor.preprocess_image(
                image, "horizontal", scale=st.session_state.upscale_factor
            )
            page_data_list.append((page_num, processed_image, page_height))
        
        # ä½¿ç”¨æ‰¹æ¬¡è™•ç†ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç©©å®šæ€§
        completed_pages = 0
        enable_postprocess = st.session_state.get('enable_text_postprocess', True)
        batch_size = st.session_state.get('batch_size', 3)
        
        # åˆå§‹åŒ–ç´¯ç©æ–‡å­—
        if 'accumulated_text' not in st.session_state:
            st.session_state.accumulated_text = ""
        
        # æ‰¹æ¬¡è™•ç†é é¢
        for i in range(0, len(page_data_list), batch_size):
            # æª¢æŸ¥æ˜¯å¦è¢«åœæ­¢
            if st.session_state.get('stop_processing', False):
                status_text.text("è™•ç†å·²åœæ­¢")
                # ä¿å­˜éƒ¨åˆ†çµæœ
                result["pages"].sort(key=lambda x: x["page_number"])
                return result, "è™•ç†è¢«ç”¨æˆ¶åœæ­¢"
            
            # ç²å–ç•¶å‰æ‰¹æ¬¡çš„é é¢
            batch_pages = page_data_list[i:i + batch_size]
            batch_results = []
            
            # è™•ç†ç•¶å‰æ‰¹æ¬¡
            for page_data in batch_pages:
                page_num, image, page_height = page_data
                
                try:
                    # è™•ç†å–®ä¸€é é¢
                    page_result, organized_texts = st.session_state.ocr_processor.process_single_page(page_data, enable_postprocess)
                    batch_results.append((page_result, organized_texts, page_num))
                    
                except Exception as e:
                    logger.error(f"è™•ç†ç¬¬ {page_num} é æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
            
            # æ‰¹æ¬¡å®Œæˆå¾Œæ›´æ–°çµæœ
            for page_result, organized_texts, page_num in batch_results:
                result["pages"].append(page_result)
                completed_pages += 1
                
                # æ·»åŠ ç•¶å‰é é¢çš„æ–‡å­—åˆ°ç´¯ç©æ–‡å­—
                page_text = f"\n=== ç¬¬ {page_num} é  ===\n"
                for block in organized_texts:
                    page_text += f"[{block['type']}] {block['text']}\n"
                st.session_state.accumulated_text += page_text
                
                # å³æ™‚é¡¯ç¤ºç´¯ç©çµæœ
                title_count = sum(1 for block in organized_texts if block["type"] == "title")
                content_count = sum(1 for block in organized_texts if block["type"] == "content")
                other_count = len(organized_texts) - title_count - content_count
                stats_line = f"å·²å®Œæˆ: {completed_pages}/{total_pages} é ï½œç•¶å‰é æ–‡æœ¬å¡Š: {len(organized_texts)}ï½œæ¨™é¡Œ: {title_count}ï½œå…§æ–‡: {content_count}ï½œå…¶ä»–: {other_count}"
                
                # åœ¨å®¹å™¨ä¸­é¡¯ç¤ºçµ±è¨ˆå’Œæ–‡å­—
                with realtime_container:
                    st.write(stats_line)
                    st.text_area("å³æ™‚æ–‡å­—çµæœ", st.session_state.accumulated_text, height=300, key=f"realtime_text_{completed_pages}", label_visibility="collapsed")
                
                # æ›´æ–°é€²åº¦æ¢ - é¡¯ç¤ºå…·é«”é æ•¸
                progress = 10 + (completed_pages / total_pages) * 80
                progress_bar.progress(int(progress))
                status_text.text(f"æ­£åœ¨è™•ç†ç¬¬ {completed_pages}/{total_pages} é ...")
            
            # æ‰¹æ¬¡å®Œæˆå¾Œæ¸…ç†è¨˜æ†¶é«”
            if completed_pages % (batch_size * 2) == 0:  # æ¯è™•ç†2å€‹æ‰¹æ¬¡å°±æ¸…ç†ä¸€æ¬¡
                st.session_state.ocr_processor._cleanup_memory()
                status_text.text(f"è¨˜æ†¶é«”æ¸…ç†å®Œæˆï¼Œç¹¼çºŒè™•ç†ç¬¬ {completed_pages + 1}/{total_pages} é ...")
                
                # çŸ­æš«æš«åœï¼Œè®“è¨˜æ†¶é«”å®Œå…¨é‡‹æ”¾
                import time
                time.sleep(0.5)
        
        # æŒ‰é ç¢¼æ’åºçµæœ
        result["pages"].sort(key=lambda x: x["page_number"])
        
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        os.unlink(tmp_file_path)
        
        status_text.text("è™•ç†å®Œæˆï¼")
        progress_bar.progress(100)
        
        return result, None
        
    except Exception as e:
        return None, f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

def display_results(result):
    """é¡¯ç¤ºè™•ç†çµæœ"""
    st.markdown("### ğŸ“Š è™•ç†çµæœæ¦‚è¦½")
    
    # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
    total_pages = result["total_pages"]
    total_text_blocks = sum(len(page["text_blocks"]) for page in result["pages"])
    
    # çµ±è¨ˆæ–‡æœ¬é¡å‹
    title_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "title") for page in result["pages"])
    content_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "content") for page in result["pages"])
    caption_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "caption") for page in result["pages"])
    table_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "table") for page in result["pages"])
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç¸½é æ•¸", total_pages)
    
    with col2:
        st.metric("æ–‡æœ¬å¡Šç¸½æ•¸", total_text_blocks)
    
    with col3:
        st.metric("æ¨™é¡Œæ•¸é‡", title_count)
    
    with col4:
        st.metric("å…§æ–‡æ•¸é‡", content_count)
    
    # æ–‡æœ¬é¡å‹åˆ†ä½ˆ
    st.markdown("### ğŸ“ˆ æ–‡æœ¬é¡å‹åˆ†ä½ˆ")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ¨™é¡Œ", title_count)
    with col2:
        st.metric("å…§æ–‡", content_count)
    with col3:
        st.metric("åœ–ç‰‡èªªæ˜", caption_count)
    with col4:
        st.metric("è¡¨æ ¼", table_count)
    
    # é é¢è©³ç´°ä¿¡æ¯
    st.markdown("### ğŸ“„ é é¢è©³ç´°ä¿¡æ¯")
    
    # å‰µå»ºé é¢é¸æ“‡å™¨
    page_options = [f"ç¬¬ {page['page_number']} é " for page in result["pages"]]
    selected_page_idx = st.selectbox("é¸æ“‡è¦æŸ¥çœ‹çš„é é¢:", range(len(page_options)), format_func=lambda x: page_options[x], key="page_selector")
    
    if selected_page_idx is not None:
        selected_page = result["pages"][selected_page_idx]
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.markdown("#### é é¢ä¿¡æ¯")
            st.write(f"**é ç¢¼:** {selected_page['page_number']}")
            st.write(f"**æ–‡æœ¬å¡Šæ•¸é‡:** {len(selected_page['text_blocks'])}")
            
            # æ–‡æœ¬é¡å‹çµ±è¨ˆ
            if selected_page["text_blocks"]:
                types = [block["type"] for block in selected_page["text_blocks"]]
                type_counts = pd.Series(types).value_counts()
                
                st.markdown("#### æ–‡æœ¬é¡å‹çµ±è¨ˆ")
                for text_type, count in type_counts.items():
                    st.write(f"**{text_type}:** {count} å€‹")
        
        with col2:
            st.markdown("#### åˆ†é¡æ–‡æœ¬å…§å®¹")
            if selected_page["text_blocks"]:
                # æŒ‰é¡å‹åˆ†çµ„é¡¯ç¤º
                for text_type in ["title", "content", "table", "caption"]:
                    type_blocks = [block for block in selected_page["text_blocks"] if block["type"] == text_type]
                    if type_blocks:
                        st.markdown(f"**{text_type.upper()}:**")
                        for block in type_blocks:
                            st.write(f"- {block['text']}")
                        st.write("")
            else:
                st.write("æ­¤é é¢æ²’æœ‰è­˜åˆ¥åˆ°æ–‡æœ¬")

def create_download_links(result):
    """å‰µå»ºä¸‹è¼‰éˆæ¥"""
    st.markdown("### ğŸ’¾ ä¸‹è¼‰çµæœ")
    
    # ç°¡åŒ–çš„JSONä¸‹è¼‰ (åªåŒ…å«æ–‡æœ¬å…§å®¹å’Œæ€§è³ª)
    simplified_result = {
        "file_name": result["file_name"],
        "total_pages": result["total_pages"],
        "pages": []
    }
    
    for page in result["pages"]:
        simplified_page = {
            "page_number": page["page_number"],
            "text_blocks": [
                {
                    "text": block["text"],
                    "type": block["type"],
                    "direction": block["direction"]
                }
                for block in page["text_blocks"]
            ]
        }
        simplified_result["pages"].append(simplified_page)
    
    json_data = json.dumps(simplified_result, ensure_ascii=False, indent=2)
    st.download_button(
        label="ğŸ“„ ä¸‹è¼‰ç°¡åŒ–JSONæ–‡ä»¶",
        data=json_data,
        file_name=f"{result['file_name']}_ocr.json",
        mime="application/json",
        key="download_json"
    )
    
    # æŒ‰é¡å‹åˆ†é¡çš„æ–‡æœ¬ä¸‹è¼‰
    classified_text = ""
    for page in result["pages"]:
        classified_text += f"=== ç¬¬ {page['page_number']} é  ===\n\n"
        
        for text_type in ["title", "content", "table", "caption"]:
            type_blocks = [block for block in page["text_blocks"] if block["type"] == text_type]
            if type_blocks:
                classified_text += f"ã€{text_type.upper()}ã€‘\n"
                for block in type_blocks:
                    classified_text += f"{block['text']}\n"
                classified_text += "\n"
        
        classified_text += "\n"
    
    st.download_button(
        label="ğŸ“ ä¸‹è¼‰åˆ†é¡æ–‡æœ¬æ–‡ä»¶",
        data=classified_text,
        file_name=f"{result['file_name']}_classified.txt",
        mime="text/plain",
        key="download_classified"
    )
    
    # ç´”æ–‡æœ¬ä¸‹è¼‰
    full_text = "\n\n".join([f"=== ç¬¬ {page['page_number']} é  ===" + "\n" + page['full_text'] for page in result["pages"]])
    st.download_button(
        label="ğŸ“„ ä¸‹è¼‰ç´”æ–‡æœ¬æ–‡ä»¶",
        data=full_text,
        file_name=f"{result['file_name']}_text.txt",
        mime="text/plain",
        key="download_text"
    )
    
    # çµ±è¨ˆå ±å‘Šä¸‹è¼‰
    title_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "title") for page in result["pages"])
    content_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "content") for page in result["pages"])
    caption_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "caption") for page in result["pages"])
    table_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "table") for page in result["pages"])
    
    stats = {
        "file_name": result["file_name"],
        "total_pages": result["total_pages"],
        "total_text_blocks": sum(len(page["text_blocks"]) for page in result["pages"]),
        "text_type_distribution": {
            "title": title_count,
            "content": content_count,
            "caption": caption_count,
            "table": table_count
        },
        "pages_detail": [
            {
                "page_number": page["page_number"],
                "text_blocks_count": len(page["text_blocks"]),
                "text_length": len(page["full_text"]),
                "text_types": {
                    text_type: sum(1 for block in page["text_blocks"] if block["type"] == text_type)
                    for text_type in ["title", "content", "table", "caption"]
                }
            }
            for page in result["pages"]
        ]
    }
    
    stats_json = json.dumps(stats, ensure_ascii=False, indent=2)
    st.download_button(
        label="ğŸ“Š ä¸‹è¼‰çµ±è¨ˆå ±å‘Š",
        data=stats_json,
        file_name=f"{result['file_name']}_stats.json",
        mime="application/json",
        key="download_stats"
    )

def main():
    """ä¸»å‡½æ•¸"""
    init_session_state()
    
    # æ¨™é¡Œ
    st.markdown('<h1 class="main-header">ğŸ“„ OCRæ–‡æœ¬è­˜åˆ¥ç³»çµ±</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">æ”¯æŒä¸­æ–‡ç›´å¼/æ©«å¼æ–‡æœ¬è­˜åˆ¥ï¼Œç”Ÿæˆçµæ§‹åŒ–JSONæ•¸æ“š</p>', unsafe_allow_html=True)
    
    # å´é‚Šæ¬„
    with st.sidebar:
        st.markdown("## ğŸ”§ ç³»çµ±è¨­ç½®")
        
        # OCRå¼•æ“ç‹€æ…‹
        if st.session_state.ocr_processor is not None:
            st.success("âœ… OCRå¼•æ“å·²å°±ç·’")
        else:
            st.warning("âš ï¸ OCRå¼•æ“æœªåˆå§‹åŒ–")
        
        # è™•ç†åƒæ•¸
        st.markdown("### è™•ç†åƒæ•¸")
        dpi = st.slider("åœ–åƒDPI", 150, 600, 500, help="æ›´é«˜çš„DPIæœƒæé«˜è­˜åˆ¥ç²¾åº¦ä½†è™•ç†æ™‚é–“æ›´é•·")
        st.session_state.upscale_factor = st.slider("è¶…è§£æå€ç‡", 1.0, 3.0, 2.5, 0.1, help=">1 æœƒå…ˆæ”¾å¤§å½±åƒå†OCRï¼Œç´°å­—æ›´æ¸…æ™°")
        st.session_state.batch_size = st.slider("æ‰¹æ¬¡è™•ç†å¤§å°", 1, 5, 3, help="æ¯æ‰¹è™•ç†çš„é æ•¸ï¼Œè¼ƒå¤§å€¼é€Ÿåº¦æ›´å¿«ä½†è¨˜æ†¶é«”ä½¿ç”¨æ›´å¤š")
        st.session_state.enable_text_postprocess = st.toggle("å•Ÿç”¨æ–‡å­—å¾Œè™•ç†ï¼ˆæ­£è¦åŒ–/è¾­å…¸ï¼‰", value=True, help="åŒ…å«NFKCã€æ•¸å­—/è‹±å­—ä¿®æ­£ã€å¯ç”¨æ™‚ç°¡è½‰ç¹")
        
        # ç³»çµ±ä¿¡æ¯
        st.markdown("### â„¹ï¸ ç³»çµ±ä¿¡æ¯")
        st.write("**æ”¯æŒæ ¼å¼:** PDF")
        st.write("**æ”¯æŒèªè¨€:** ä¸­æ–‡ï¼ˆç°¡é«”/ç¹é«”ï¼‰")
        st.write("**æ–‡æœ¬æ–¹å‘:** ç›´å¼/æ©«å¼")
        st.write("**OCRå¼•æ“:** Tesseract (å¢å¼·ç‰ˆ)")
        st.info("â„¹ï¸ åœ¨Streamlit Cloudç’°å¢ƒä¸­ï¼Œä½¿ç”¨å¢å¼·ç‰ˆTesseracté€²è¡ŒOCRè™•ç†")
        
        # æ¸…é™¤ç‹€æ…‹æŒ‰éˆ•
        st.markdown("### ğŸ”§ ç³»çµ±æ§åˆ¶")
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç‹€æ…‹", type="secondary"):
            # æ¸…é™¤æ‰€æœ‰session state
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ ä¸Šå‚³è™•ç†", "ğŸ“Š çµæœæŸ¥çœ‹", "ğŸ“š ä½¿ç”¨èªªæ˜"])
    
    with tab1:
        st.markdown("### ğŸ“¤ ä¸Šå‚³PDFæ–‡ä»¶")
        
        # æ–‡ä»¶ä¸Šå‚³
        uploaded_file = st.file_uploader(
            "é¸æ“‡PDFæ–‡ä»¶",
            type=['pdf'],
            help="æ”¯æŒä¸­æ–‡ç›´å¼å’Œæ©«å¼æ–‡æœ¬çš„PDFæ–‡ä»¶"
        )
        
        if uploaded_file is not None:
            # é¡¯ç¤ºæ–‡ä»¶ä¿¡æ¯
            st.markdown('<div class="file-info">', unsafe_allow_html=True)
            st.write(f"**æ–‡ä»¶å:** {uploaded_file.name}")
            st.write(f"**æ–‡ä»¶å¤§å°:** {uploaded_file.size / 1024 / 1024:.2f} MB")
            st.markdown('</div>', unsafe_allow_html=True)
            
            # è™•ç†æ§åˆ¶æŒ‰éˆ•
            col1, col2, col3 = st.columns([1, 1, 2])
            
            with col1:
                if not st.session_state.is_processing:
                    if st.button("ğŸš€ é–‹å§‹OCRè™•ç†", type="primary"):
                        # åˆå§‹åŒ–OCRå¼•æ“
                        if not load_ocr_processor():
                            st.stop()
                        
                        # è¨­ç½®è™•ç†ç‹€æ…‹
                        st.session_state.is_processing = True
                        st.session_state.stop_processing = False
                        # é‡ç½®ç´¯ç©æ–‡å­—
                        st.session_state.accumulated_text = ""
                        st.rerun()
                else:
                    st.button("â¸ï¸ è™•ç†ä¸­...", disabled=True)
            
            with col2:
                if st.session_state.is_processing:
                    if st.button("â¹ï¸ åœæ­¢è™•ç†", type="secondary", key="stop_processing_btn"):
                        st.session_state.stop_processing = True
                        st.session_state.is_processing = False
                        st.rerun()
            
            # è™•ç†é€²åº¦å’Œå³æ™‚çµæœ
            if st.session_state.is_processing:
                # å‰µå»ºé€²åº¦æ¢å’Œç‹€æ…‹é¡¯ç¤ºï¼ˆå”¯ä¸€placeholderï¼Œé¿å…é‡è¤‡ï¼‰
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # å³æ™‚çµæœé¡¯ç¤ºå€åŸŸ - ä½¿ç”¨å¯æ»¾å‹•å®¹å™¨
                st.markdown("### ğŸ“Š å³æ™‚è™•ç†çµæœ")
                realtime_container = st.container()
                
                # è™•ç†æ–‡ä»¶
                result, error = process_pdf_file(uploaded_file, progress_bar, status_text, realtime_container)
                
                # è™•ç†å®Œæˆ
                st.session_state.is_processing = False
                
                if error:
                    if "è™•ç†è¢«ç”¨æˆ¶åœæ­¢" in error:
                        st.warning("âš ï¸ è™•ç†å·²åœæ­¢")
                        
                        # å¦‚æœæœ‰éƒ¨åˆ†çµæœï¼Œæä¾›é¸é …
                        if result["pages"]:
                            st.session_state.processing_results[uploaded_file.name] = result
                            st.session_state.current_file = uploaded_file.name
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                if st.button("ğŸ”„ é‡æ–°è™•ç†", type="primary", key="restart_processing"):
                                    # é‡ç½®æ‰€æœ‰è™•ç†ç‹€æ…‹
                                    st.session_state.stop_processing = False
                                    st.session_state.is_processing = False
                                    st.session_state.accumulated_text = ""
                                    st.rerun()
                            with col2:
                                if st.button("ğŸ“¥ åŒ¯å‡ºéƒ¨åˆ†çµæœ", type="secondary", key="export_partial"):
                                    st.session_state.stop_processing = False
                                    st.session_state.is_processing = False
                                    st.rerun()
                            with col3:
                                if st.button("â­ï¸ ç¹¼çºŒè™•ç†", type="secondary", key="continue_processing"):
                                    st.session_state.stop_processing = False
                                    st.session_state.is_processing = True
                                    st.rerun()
                            
                            # é¡¯ç¤ºéƒ¨åˆ†çµæœ
                            st.markdown("### ğŸ“Š éƒ¨åˆ†è™•ç†çµæœ")
                            display_results(result)
                            create_download_links(result)
                        else:
                            st.info("æ²’æœ‰è™•ç†ä»»ä½•é é¢ï¼Œè«‹é‡æ–°é–‹å§‹è™•ç†ã€‚")
                            # é‡ç½®ç‹€æ…‹
                            st.session_state.stop_processing = False
                            st.session_state.is_processing = False
                    else:
                        st.error(f"âŒ {error}")
                        # é‡ç½®ç‹€æ…‹
                        st.session_state.stop_processing = False
                        st.session_state.is_processing = False
                else:
                    st.session_state.stop_processing = False
                    st.session_state.is_processing = False
                    st.success("âœ… è™•ç†å®Œæˆï¼")
                    
                    # ä¿å­˜çµæœåˆ°session state
                    st.session_state.processing_results[uploaded_file.name] = result
                    st.session_state.current_file = uploaded_file.name
                    
                    # é¡¯ç¤ºæœ€çµ‚çµæœ
                    display_results(result)
                    create_download_links(result)
    
    with tab2:
        st.markdown("### ğŸ“Š æŸ¥çœ‹è™•ç†çµæœ")
        
        if st.session_state.processing_results:
            # æ–‡ä»¶é¸æ“‡å™¨
            file_names = list(st.session_state.processing_results.keys())
            selected_file = st.selectbox("é¸æ“‡å·²è™•ç†çš„æ–‡ä»¶:", file_names, key="file_selector")
            
            if selected_file:
                result = st.session_state.processing_results[selected_file]
                
                # ç°¡åŒ–çš„çµæœæ¦‚è¦½
                st.markdown("### ğŸ“ˆ çµæœæ¦‚è¦½")
                col1, col2, col3, col4 = st.columns(4)
                
                total_pages = result["total_pages"]
                total_text_blocks = sum(len(page["text_blocks"]) for page in result["pages"])
                title_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "title") for page in result["pages"])
                content_count = sum(sum(1 for block in page["text_blocks"] if block["type"] == "content") for page in result["pages"])
                
                with col1:
                    st.metric("ç¸½é æ•¸", total_pages)
                with col2:
                    st.metric("æ–‡æœ¬å¡Šç¸½æ•¸", total_text_blocks)
                with col3:
                    st.metric("æ¨™é¡Œæ•¸é‡", title_count)
                with col4:
                    st.metric("å…§æ–‡æ•¸é‡", content_count)
                
                # é è¦½å€åŸŸ - ä½¿ç”¨å¯æ»¾å‹•å®¹å™¨
                st.markdown("### ğŸ‘€ å…§å®¹é è¦½")
                
                # å‰µå»ºé è¦½é¸é …
                preview_type = st.radio("é è¦½é¡å‹:", ["JSONæ ¼å¼", "åˆ†é¡æ–‡æœ¬", "ç´”æ–‡æœ¬"], horizontal=True, key="preview_type")
                
                # ä½¿ç”¨å®¹å™¨å‰µå»ºå¯æ»¾å‹•å€åŸŸ
                with st.container():
                    if preview_type == "JSONæ ¼å¼":
                        # å®Œæ•´çš„JSONé è¦½
                        simplified_result = {
                            "file_name": result["file_name"],
                            "total_pages": result["total_pages"],
                            "pages": []
                        }
                        
                        for page in result["pages"]:
                            simplified_page = {
                                "page_number": page["page_number"],
                                "text_blocks": [
                                    {
                                        "text": block["text"],
                                        "type": block["type"],
                                        "direction": block["direction"]
                                    }
                                    for block in page["text_blocks"]
                                ]
                            }
                            simplified_result["pages"].append(simplified_page)
                        
                        # ä½¿ç”¨å¯æ»¾å‹•çš„JSONé¡¯ç¤º
                        st.json(simplified_result)
                    
                    elif preview_type == "åˆ†é¡æ–‡æœ¬":
                        # å®Œæ•´çš„åˆ†é¡æ–‡æœ¬é è¦½
                        preview_text = ""
                        for page in result["pages"]:
                            preview_text += f"=== ç¬¬ {page['page_number']} é  ===\n"
                            for text_type in ["title", "content", "table", "caption"]:
                                type_blocks = [block for block in page["text_blocks"] if block["type"] == text_type]
                                if type_blocks:
                                    preview_text += f"ã€{text_type.upper()}ã€‘\n"
                                    for block in type_blocks:
                                        preview_text += f"{block['text']}\n"
                                    preview_text += "\n"
                            preview_text += "\n"
                        
                        # ä½¿ç”¨å¯æ»¾å‹•çš„æ–‡æœ¬å€åŸŸ
                        st.text_area("classified_preview", preview_text, height=500, key="classified_preview", label_visibility="collapsed")
                    
                    else:  # ç´”æ–‡æœ¬
                        preview_text = ""
                        for page in result["pages"]:
                            preview_text += f"=== ç¬¬ {page['page_number']} é  ===\n"
                            preview_text += page['full_text']
                            preview_text += "\n\n"
                        
                        # ä½¿ç”¨å¯æ»¾å‹•çš„æ–‡æœ¬å€åŸŸ
                        st.text_area("text_preview", preview_text, height=500, key="text_preview", label_visibility="collapsed")
                
                # ä¸‹è¼‰æŒ‰éˆ•
                create_download_links(result)
        else:
            st.info("å°šæœªè™•ç†ä»»ä½•æ–‡ä»¶ï¼Œè«‹å…ˆä¸Šå‚³ä¸¦è™•ç†PDFæ–‡ä»¶ã€‚")
    
    with tab3:
        st.markdown("### ğŸ“š ä½¿ç”¨èªªæ˜")
        
        st.markdown("""
        #### ğŸ¯ åŠŸèƒ½ç‰¹é»
        
        - **é›™å¼•æ“OCR**: çµåˆPaddleOCRå’ŒTesseractï¼Œæé«˜è­˜åˆ¥æº–ç¢ºåº¦
        - **æ–¹å‘æª¢æ¸¬**: è‡ªå‹•æª¢æ¸¬ç›´å¼/æ©«å¼æ–‡æœ¬
        - **ä¸­æ–‡å„ªåŒ–**: é‡å°ä¸­æ–‡å­—é«”å„ªåŒ–
        - **çµæ§‹åŒ–è¼¸å‡º**: ç”Ÿæˆè©³ç´°çš„JSONæ•¸æ“šï¼ŒåŒ…å«ä½ç½®ä¿¡æ¯
        
        #### ğŸ“‹ ä½¿ç”¨æ­¥é©Ÿ
        
        1. **ä¸Šå‚³æ–‡ä»¶**: åœ¨"ä¸Šå‚³è™•ç†"æ¨™ç±¤é é¸æ“‡PDFæ–‡ä»¶
        2. **é–‹å§‹è™•ç†**: é»æ“Š"é–‹å§‹OCRè™•ç†"æŒ‰éˆ•
        3. **æŸ¥çœ‹çµæœ**: åœ¨"çµæœæŸ¥çœ‹"æ¨™ç±¤é æŸ¥çœ‹è™•ç†çµæœ
        4. **ä¸‹è¼‰æ•¸æ“š**: ä¸‹è¼‰JSONã€æ–‡æœ¬æˆ–çµ±è¨ˆå ±å‘Š
        
        #### ğŸ“Š è¼¸å‡ºæ ¼å¼
        
        æ¯å€‹PDFæœƒç”ŸæˆåŒ…å«ä»¥ä¸‹ä¿¡æ¯çš„JSONæ–‡ä»¶ï¼š
        
        - **æ–‡ä»¶åŸºæœ¬ä¿¡æ¯**: æ–‡ä»¶åã€ç¸½é æ•¸
        - **é é¢è©³ç´°ä¿¡æ¯**: æ¯é çš„æ–‡æœ¬æ–¹å‘ã€æ–‡æœ¬å¡Šæ•¸é‡
        - **æ–‡æœ¬å…§å®¹**: è­˜åˆ¥çš„æ–‡å­—å…§å®¹å’Œä½ç½®ä¿¡æ¯
        - **ç½®ä¿¡åº¦**: OCRè­˜åˆ¥çš„ç½®ä¿¡åº¦åˆ†æ•¸
        
        #### âš™ï¸ æŠ€è¡“åƒæ•¸
        
        - **åœ–åƒDPI**: å¯èª¿æ•´ï¼Œå»ºè­°300-600
        - **æ–‡æœ¬æ–¹å‘**: è‡ªå‹•æª¢æ¸¬ç›´å¼/æ©«å¼
        - **èªè¨€æ”¯æŒ**: ç°¡é«”ä¸­æ–‡ã€ç¹é«”ä¸­æ–‡ã€è‹±æ–‡
        - **è™•ç†æ™‚é–“**: æ ¹æ“šæ–‡ä»¶å¤§å°å’Œé æ•¸è€Œå®š
        
        #### ğŸ”§ ç³»çµ±è¦æ±‚
        
        - æ”¯æŒPDFæ ¼å¼æ–‡ä»¶
        - å»ºè­°æ–‡ä»¶å¤§å°ä¸è¶…é100MB
        - è™•ç†æ™‚é–“èˆ‡æ–‡ä»¶å¤§å°æˆæ­£æ¯”
        """)

if __name__ == "__main__":
    main()